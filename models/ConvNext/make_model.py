import torch
import torch.nn as nn
from torch.autograd import Variable
import torch.nn.functional as F
from timm.models import create_model
from .backbones.model_convnext import convnext_tiny
from .backbones.resnet import Resnet
import numpy as np
from torch.nn import init
from torch.nn.parameter import Parameter


class Gem_heat(nn.Module):
    def __init__(self, dim = 768, p=3, eps=1e-6):
        super(Gem_heat, self).__init__()
        self.p = nn.Parameter(torch.ones(dim) * p) 
        self.eps = eps

    def forward(self, x):
        return self.gem(x, p=self.p, eps=self.eps)


    def gem(self, x, p=3):
        p = F.softmax(p).unsqueeze(-1)
        x = torch.matmul(x,p)
        x = x.view(x.size(0), x.size(1))
        return x


def position(H, W, is_cuda=True):
    if is_cuda:
        loc_w = torch.linspace(-1.0, 1.0, W).cuda().unsqueeze(0).repeat(H, 1)
        loc_h = torch.linspace(-1.0, 1.0, H).cuda().unsqueeze(1).repeat(1, W)
    else:
        loc_w = torch.linspace(-1.0, 1.0, W).unsqueeze(0).repeat(H, 1)
        loc_h = torch.linspace(-1.0, 1.0, H).unsqueeze(1).repeat(1, W)
    loc = torch.cat([loc_w.unsqueeze(0), loc_h.unsqueeze(0)], 0).unsqueeze(0)
    return loc


def stride(x, stride):
    b, c, h, w = x.shape
    return x[:, :, ::stride, ::stride]


def init_rate_half(tensor):
    if tensor is not None:
        tensor.data.fill_(0.5)


def init_rate_0(tensor):
    if tensor is not None:
        tensor.data.fill_(0.)


class BasicConv(nn.Module):
    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):
        super(BasicConv, self).__init__()
        self.out_channels = out_planes
        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)
        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None
        self.relu = nn.ReLU() if relu else None

    def forward(self, x):
        x = self.conv(x)
        if self.bn is not None:
            x = self.bn(x)
        if self.relu is not None:
            x = self.relu(x)
        return x

class ZPool(nn.Module):
    def forward(self, x):
        return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1)

class AttentionGate(nn.Module):
    def __init__(self):
        super(AttentionGate, self).__init__()
        kernel_size = 7
        self.compress = ZPool()
        self.conv = BasicConv(2, 1, kernel_size, stride=1, padding=(kernel_size-1) // 2, relu=False)
    def forward(self, x):
        x_compress = self.compress(x)
        x_out = self.conv(x_compress)
        scale = torch.sigmoid_(x_out)
        return x * scale

class TripletAttention(nn.Module):
    def __init__(self):
        super(TripletAttention, self).__init__()
        self.cw = AttentionGate()
        self.hc = AttentionGate()
    def forward(self, x):
        x_perm1 = x.permute(0,2,1,3).contiguous()
        x_out1 = self.cw(x_perm1)
        x_out11 = x_out1.permute(0,2,1,3).contiguous()
        x_perm2 = x.permute(0,3,2,1).contiguous()
        x_out2 = self.hc(x_perm2)
        x_out21 = x_out2.permute(0,3,2,1).contiguous()
        return x_out11, x_out21


class ClassBlock(nn.Module):
    """
    åˆ†ç±»å™¨å—
    åŒ…å«ï¼šå…¨è¿æ¥å±‚ â†’ BN â†’ ReLU â†’ Dropout â†’ åˆ†ç±»å™¨
    """
    def __init__(self, input_dim, class_num, droprate=0.5, relu=False, 
                 bnorm=True, num_bottleneck=512, linear=True, return_f=False):
        super(ClassBlock, self).__init__()
        self.return_f = return_f
        add_block = []
        
        if linear:
            add_block += [nn.Linear(input_dim, num_bottleneck)]
        else:
            num_bottleneck = input_dim
        
        if bnorm:
            add_block += [nn.BatchNorm1d(num_bottleneck)]
        if relu:
            add_block += [nn.LeakyReLU(0.1)]
        if droprate > 0:
            add_block += [nn.Dropout(p=droprate)]
        
        add_block = nn.Sequential(*add_block)
        add_block.apply(self.weights_init_kaiming)
        
        classifier = []
        classifier += [nn.Linear(num_bottleneck, class_num)]
        classifier = nn.Sequential(*classifier)
        classifier.apply(self.weights_init_classifier)
        
        self.add_block = add_block
        self.classifier = classifier
    
    def weights_init_kaiming(self, m):
        classname = m.__class__.__name__
        if classname.find('Linear') != -1:
            nn.init.kaiming_normal_(m.weight, a=0, mode='fan_out')
            nn.init.constant_(m.bias, 0.0)
        elif classname.find('Conv') != -1:
            nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')
            if m.bias is not None:
                nn.init.constant_(m.bias, 0.0)
        elif classname.find('BatchNorm') != -1:
            if m.affine:
                nn.init.constant_(m.weight, 1.0)
                nn.init.constant_(m.bias, 0.0)
    
    def weights_init_classifier(self, m):
        """åˆ†ç±»å™¨æƒé‡åˆå§‹åŒ–"""
        classname = m.__class__.__name__
        if classname.find('Linear') != -1:
            nn.init.normal_(m.weight, std=0.001)
            if m.bias is not None:  # âœ… ä¿®å¤ï¼šæ­£ç¡®æ£€æŸ¥ bias
                nn.init.constant_(m.bias, 0.0)
    
    def forward(self, x):
        x = self.add_block(x)
        if self.return_f:
            f = x
            x = self.classifier(x)
            return [x, f]
        else:
            x = self.classifier(x)
            return x


def weights_init_kaiming(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_out')
        nn.init.constant_(m.bias, 0.0)

    elif classname.find('Conv') != -1:
        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')
        if m.bias is not None:
            nn.init.constant_(m.bias, 0.0)
    elif classname.find('BatchNorm') != -1:
        if m.affine:
            nn.init.constant_(m.weight, 1.0)
            nn.init.constant_(m.bias, 0.0)

def weights_init_classifier(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight.data, std=0.001)
        nn.init.constant_(m.bias.data, 0.0)

class build_convnext(nn.Module):
    """
    ConvNeXt/ResNet éª¨å¹²ç½‘ç»œ + TripletAttention + å¤šåˆ†ç±»å™¨
    
    âœ… æ”¯æŒå•è¾“å…¥å’ŒåŒè¾“å…¥
    âœ… æ”¯æŒæ‰€æœ‰å‚æ•°ï¼ˆå‘åå…¼å®¹ï¼‰
    âœ… æ”¯æŒé›¶åˆå§‹åŒ–æ¨¡å¼
    """
    def __init__(
        self, 
        num_classes, 
        block=4, 
        return_f=False, 
        resnet=False,
        # â­ æ–°å‚æ•°ï¼ˆå…¼å®¹é›¶åˆå§‹åŒ–ï¼‰
        backbone_type=None,
        dropout=0.5,
        attention_type='none',
        attention_config=None,
        **kwargs  # æ•è·å…¶ä»–å‚æ•°
    ):
        super(build_convnext, self).__init__()
        
        self.return_f = return_f
        self.block = block
        self.num_classes = num_classes
        
        # ========== å¤„ç† backbone ç±»å‹ ==========
        if backbone_type is None:
            # å‘åå…¼å®¹ï¼šå¦‚æœæ²¡æœ‰æŒ‡å®š backbone_typeï¼Œä½¿ç”¨ resnet å‚æ•°
            if resnet:
                backbone_type = 'resnet'
            else:
                backbone_type = 'convnext'
        
        # ========== åˆå§‹åŒ– Backbone ==========
        if backbone_type == 'resnet' or resnet:
            convnext_name = "resnet101"
            print('using model_type: {} as a backbone'.format(convnext_name))
            self.in_planes = 2048
            
            try:
                from models.resnet_backbone import Resnet
                self.convnext = Resnet(pretrained=True)
            except ImportError:
                print('[WARNING] Cannot import Resnet, using ConvNeXt instead')
                convnext_name = "convnext_tiny"
                self.in_planes = 768
                self.convnext = create_model(convnext_name, pretrained=True)
        
        else:  # convnext
            convnext_name = "convnext_tiny"
            print('using model_type: {} as a backbone'.format(convnext_name))
            
            if 'base' in convnext_name:
                self.in_planes = 1024
            elif 'large' in convnext_name:
                self.in_planes = 1536
            elif 'xlarge' in convnext_name:
                self.in_planes = 2048
            else:
                self.in_planes = 768
            
            self.convnext = create_model(convnext_name, pretrained=True)
        
        # ========== åˆ†ç±»å™¨ ==========
        dropout_rate = dropout if dropout is not None else 0.5
        
        self.classifier1 = ClassBlock(
            self.in_planes, 
            num_classes, 
            dropout_rate, 
            return_f=return_f
        )
        
        # ========== TripletAttention ==========
        # ========== TripletAttention å¯¼å…¥ï¼ˆä¿®å¤ï¼‰==========
        try:
            # å°è¯•æ–¹æ¡ˆ 1: ç»å¯¹å¯¼å…¥
            from models.ConvNext.backbones.triplet_attention import TripletAttention
            self.tri_layer = TripletAttention()
            print("[INFO] TripletAttention loaded successfully")
        except ImportError:
            try:
                # å°è¯•æ–¹æ¡ˆ 2: ç›¸å¯¹å¯¼å…¥
                from .backbones.triplet_attention import TripletAttention
                self.tri_layer = TripletAttention()
                print("[INFO] TripletAttention loaded successfully (relative import)")
            except ImportError:
                try:
                    # å°è¯•æ–¹æ¡ˆ 3: ç›´æ¥ä» ConvNext å¯¼å…¥
                    from ConvNext.backbones.triplet_attention import TripletAttention
                    self.tri_layer = TripletAttention()
                    print("[INFO] TripletAttention loaded successfully (ConvNext import)")
                except ImportError:
                    print("[ERROR] Cannot import TripletAttention from any path")
                    print("       Creating dummy TripletAttention...")
                    
                    # âœ… åˆ›å»ºå…¼å®¹çš„ dummy TripletAttention
                    class DummyTripletAttention(nn.Module):
                        """
                        Dummy TripletAttention - è¿”å›ä¸çœŸå® TripletAttention ç›¸åŒçš„æ ¼å¼
                        """
                        def __init__(self):
                            super().__init__()
                        
                        def forward(self, x):
                            """
                            è¿”å› listï¼Œä¸çœŸå® TripletAttention æ ¼å¼ä¸€è‡´
                            
                            Args:
                                x: [B, C, H, W]
                            
                            Returns:
                                list of 2 tensors: [x, x]ï¼ˆä¸¤ä¸ªåˆ†æ”¯ï¼‰
                            """
                            # è¿”å›ä¸¤ä¸ªç›¸åŒçš„ç‰¹å¾ï¼ˆæ¨¡æ‹Ÿä¸¤ä¸ªæ³¨æ„åŠ›åˆ†æ”¯ï¼‰
                            return [x, x]
                    
                    self.tri_layer = DummyTripletAttention()
                    print("[INFO] Using DummyTripletAttention (returns list of 2 features)")
        
        # ========== å¤šåˆ†ç±»å™¨ï¼ˆMCBï¼‰==========
        for i in range(self.block):
            name = 'classifier_mcb' + str(i + 1)
            setattr(self, name, ClassBlock(
                self.in_planes, 
                num_classes, 
                dropout_rate, 
                return_f=self.return_f
            ))
    
    def part_classifier(self, block, x, cls_name='classifier'):
        """å¤šåˆ†ç±»å™¨å¤„ç†"""
        part = {}
        for i in range(block):
            part[i] = x[:, :, i].view(x.size(0), -1)
            name = cls_name + str(i + 1)
            c = getattr(self, name)
            part[i] = c(part[i])
        y = []
        for i in range(block):
            y.append(part[i])
        return y
    
    def forward(self, x, x2=None, return_original_feat=False):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: ç¬¬ä¸€ä¸ªè¾“å…¥ï¼ˆsatelliteï¼‰[B, 3, 256, 256]
            x2: ç¬¬äºŒä¸ªè¾“å…¥ï¼ˆdroneï¼‰[B, 3, 256, 256]ï¼Œå¯é€‰
            return_original_feat: ä¿ç•™å‚æ•°ï¼ˆç”¨äºé›¶åˆå§‹åŒ–å…¼å®¹ï¼‰
        
        Returns:
            è®­ç»ƒæ¨¡å¼ + åŒè¾“å…¥: ((cls1, feat1), (cls2, feat2))
            è®­ç»ƒæ¨¡å¼ + å•è¾“å…¥: (cls, feat) æˆ– y
            æµ‹è¯•æ¨¡å¼ + åŒè¾“å…¥: (y1, y2)
            æµ‹è¯•æ¨¡å¼ + å•è¾“å…¥: y
        """
        
        # ========== å¤„ç†ç¬¬ä¸€ä¸ªè¾“å…¥ï¼ˆsatelliteï¼‰==========
        gap_feature, part_features = self.convnext(x)
        tri_features = self.tri_layer(part_features)
        convnext_feature = self.classifier1(gap_feature)

        tri_list = []
        for i in range(len(tri_features)):
            tri_list.append(tri_features[i].mean([-2, -1]))
        
        # âœ… ä¿®å¤ï¼šç¡®ä¿æœ‰è¶³å¤Ÿçš„ç‰¹å¾
        while len(tri_list) < self.block:
            if len(tri_list) > 0:
                tri_list.append(tri_list[0])
            else:
                # å¦‚æœ tri_features ä¸ºç©ºï¼Œåˆ›å»ºé›¶å¼ é‡
                tri_list.append(torch.zeros_like(gap_feature))
        
        triatten_features = torch.stack(tri_list[:self.block], dim=2)
        
        if self.block == 0:
            y = []
        else:
            y = self.part_classifier(self.block, triatten_features, cls_name='classifier_mcb')

        # ========== å¤„ç†ç¬¬äºŒä¸ªè¾“å…¥ï¼ˆdroneï¼Œå¦‚æœæœ‰ï¼‰==========
        if x2 is not None:
            gap_feature2, part_features2 = self.convnext(x2)
            tri_features2 = self.tri_layer(part_features2)
            convnext_feature2 = self.classifier1(gap_feature2)

            tri_list2 = []
            for i in range(len(tri_features2)):
                tri_list2.append(tri_features2[i].mean([-2, -1]))
            
            # âœ… ä¿®å¤ï¼šç¡®ä¿æœ‰è¶³å¤Ÿçš„ç‰¹å¾
            while len(tri_list2) < self.block:
                if len(tri_list2) > 0:
                    tri_list2.append(tri_list2[0])
                else:
                    tri_list2.append(torch.zeros_like(gap_feature2))
            
            triatten_features2 = torch.stack(tri_list2[:self.block], dim=2)
            
            if self.block == 0:
                y2 = []
            else:
                y2 = self.part_classifier(self.block, triatten_features2, cls_name='classifier_mcb')

        # ========== è¿”å›ç»“æœ ==========
        if self.training:
            # è®­ç»ƒæ¨¡å¼
            y = y + [convnext_feature]
            
            if x2 is not None:
                y2 = y2 + [convnext_feature2]
            
            if self.return_f:
                # è¿”å›åˆ†ç±»å’Œç‰¹å¾ï¼ˆç”¨äº triplet lossï¼‰
                cls, features = [], []
                for i in y:
                    cls.append(i[0])
                    features.append(i[1])
                
                if x2 is not None:
                    cls2, features2 = [], []
                    for i in y2:
                        cls2.append(i[0])
                        features2.append(i[1])
                    
                    # âœ… åŒè¾“å…¥è®­ç»ƒ
                    return (cls, features), (cls2, features2)
                else:
                    # å•è¾“å…¥è®­ç»ƒ
                    return (cls, features)
            else:
                # ä¸è¿”å›ç‰¹å¾
                if x2 is not None:
                    return y, y2
                else:
                    return y
        
        else:
            # âœ… æµ‹è¯•æ¨¡å¼
            ffeature = convnext_feature.view(convnext_feature.size(0), -1, 1)
            
            if self.block == 0:
                y_out = ffeature
            else:
                y_out = torch.cat([y, ffeature], dim=2)
            
            if x2 is not None:
                ffeature2 = convnext_feature2.view(convnext_feature2.size(0), -1, 1)
                
                if self.block == 0:
                    y2_out = ffeature2
                else:
                    y2_out = torch.cat([y2, ffeature2], dim=2)
                
                # âœ… åŒè¾“å…¥æµ‹è¯•
                return y_out, y2_out
            else:
                # å•è¾“å…¥æµ‹è¯•
                return y_out

    def part_classifier(self, block, x, cls_name='classifier_mcb'):
        part = {}
        predict = {}
        for i in range(block):
            part[i] = x[:, :, i].view(x.size(0), -1)
            name = cls_name + str(i+1)
            c = getattr(self, name)
            predict[i] = c(part[i])
        y = []
        for i in range(block):
            y.append(predict[i])
        if not self.training:
            return torch.stack(y, dim=2)
        return y


def make_convnext_model(num_class,block = 4,return_f=False,resnet=False):
    print('===========building convnext===========')
    model = build_convnext(num_class,block=block,return_f=return_f,resnet=resnet)
    return model

# ============================================================================
# â­ é›¶åˆå§‹åŒ–æ”¯æŒï¼ˆæ·»åŠ åˆ°æ–‡ä»¶æœ«å°¾ï¼‰
# ============================================================================

def make_model_with_zero_init(
    num_class,
    block=4,
    return_f=False,
    backbone='convnext',
    dinov2_model='vitb14',
    freeze_dinov2=False,
    use_structure_aware=False,
    use_hybrid=False,
    dropout=0.5,
    # ========== é›¶åˆå§‹åŒ–å‚æ•° ==========
    use_zero_init=False,
    use_zero_init_tri=False,
    use_zero_init_detail=False,
    use_zero_init_aff=False,
):
    """
    æ”¯æŒé›¶åˆå§‹åŒ–çš„æ¨¡å‹åˆ›å»ºå‡½æ•°
    
    Args:
        num_class: ç±»åˆ«æ•°
        block: MCCG åˆ†ç±»å™¨æ•°é‡
        return_f: æ˜¯å¦è¿”å›ç‰¹å¾
        backbone: backbone ç±»å‹
        ... (å…¶ä»–å‚æ•°ä¿æŒä¸å˜)
        
        use_zero_init: æ˜¯å¦å¯ç”¨é›¶åˆå§‹åŒ–
        use_zero_init_tri: é›¶åˆå§‹åŒ– TripletAttention
        use_zero_init_detail: é›¶åˆå§‹åŒ– DetailBranch
        use_zero_init_aff: é›¶åˆå§‹åŒ– AFF
    
    Returns:
        model: æ¨¡å‹å®ä¾‹
    """
    
    # ========== æ£€æŸ¥é›¶åˆå§‹åŒ– ==========
    if use_zero_init:
        print("\n" + "="*80)
        print("ğŸ”¥ Zero-Initialization Mode ENABLED")
        print("="*80)
        print(f"  - Zero-Init TripletAttention: {use_zero_init_tri}")
        print(f"  - Zero-Init DetailBranch: {use_zero_init_detail}")
        print(f"  - Zero-Init AFF: {use_zero_init_aff}")
        print("="*80 + "\n")
        
        try:
            # å¯¼å…¥é›¶åˆå§‹åŒ–æ¨¡å—
            from models.zeroInit_modules import ZeroInitMCCG
            
            model = ZeroInitMCCG(
                num_classes=num_class,
                block=block,
                use_zero_init_tri=use_zero_init_tri,
                use_zero_init_detail=use_zero_init_detail,
                use_zero_init_aff=use_zero_init_aff
            )
            
            print("âœ… Zero-Init MCCG model created successfully\n")
            return model
            
        except ImportError as e:
            print(f"âŒ Error: Cannot import zeroInit_modules")
            print(f"   {e}")
            print("   Falling back to standard model...\n")
            use_zero_init = False
    
    # ========== æ ‡å‡†æ¨¡å‹ï¼ˆåŸé€»è¾‘ï¼‰==========
    print(f"\n{'='*80}")
    print(f"Creating Standard MCCG Model with {backbone.upper()} Backbone")
    print(f"{'='*80}")
    
    if backbone == 'dinov2':
        print(f"  - Model size: {dinov2_model}")
        print(f"  - Freeze backbone: {freeze_dinov2}")
        print(f"  - Dropout: {dropout}")
        
        if use_hybrid:
            print(f"  - ğŸ”¥ Hybrid Feature Extraction: ENABLED")
        else:
            print(f"  - â­• Hybrid Feature Extraction: DISABLED")
        
        if use_structure_aware:
            print(f"  - ğŸ”¥ Structure-Aware Module: ENABLED")
        else:
            print(f"  - â­• Structure-Aware Module: DISABLED")
    
    print(f"{'='*80}\n")
    
    # â­â­â­ å…³é”®ä¿®å¤ï¼šè°ƒç”¨æ­£ç¡®çš„å‡½æ•° â­â­â­
    # âŒ åŸæ¥è°ƒç”¨ build_convnext ä¼šå‡ºé”™ï¼Œå› ä¸ºå‚æ•°ä¸åŒ¹é…
    # âœ… åº”è¯¥è°ƒç”¨ make_convnext_modelï¼ˆå·²å­˜åœ¨çš„å‡½æ•°ï¼‰
    
    if backbone == 'dinov2':
        # DINOv2 æ¨¡å‹
        # æ³¨æ„ï¼šä½ çš„ä»£ç ä¸­å¯èƒ½æ²¡æœ‰ make_dinov2_modelï¼Œéœ€è¦æ£€æŸ¥
        try:
            model = make_dinov2_model(
                num_class=num_class,
                block=block,
                return_f=return_f,
                model_size=dinov2_model
            )
        except NameError:
            print("[WARNING] make_dinov2_model not found, using ConvNeXt instead")
            model = make_convnext_model(
                num_class=num_class,
                block=block,
                return_f=return_f,
                resnet=False
            )
    
    elif backbone == 'resnet':
        # ResNet æ¨¡å‹
        model = make_convnext_model(
            num_class=num_class,
            block=block,
            return_f=return_f,
            resnet=True
        )
    
    else:  # convnextï¼ˆé»˜è®¤ï¼‰
        # ConvNeXt æ¨¡å‹
        model = make_convnext_model(
            num_class=num_class,
            block=block,
            return_f=return_f,
            resnet=False
        )
    
    return model


def make_model_from_opt(opt):
    """
    ä» opt å¯¹è±¡è‡ªåŠ¨åˆ›å»ºæ¨¡å‹
    
    â­ æ¨èä½¿ç”¨æ­¤å‡½æ•°ï¼Œè‡ªåŠ¨æ£€æµ‹æ‰€æœ‰å‚æ•°
    
    Args:
        opt: è®­ç»ƒå‚æ•°å¯¹è±¡
    
    Returns:
        model: æ¨¡å‹å®ä¾‹
    
    ä½¿ç”¨ç¤ºä¾‹:
        # train.py ä¸­
        from models.ConvNext.make_model import make_model_from_opt
        model = make_model_from_opt(opt)
    """
    
    # â­ å…³é”®ä¿®å¤ï¼šæ­£ç¡®æ£€æµ‹ backbone ç±»å‹
    if getattr(opt, 'dinov2', False):
        backbone = 'dinov2'
    elif getattr(opt, 'resnet', False):
        backbone = 'resnet'
    else:
        backbone = 'convnext'
    
    return make_model_with_zero_init(
        num_class=opt.nclasses,
        block=opt.block,
        return_f=True,  # è®­ç»ƒæ—¶æ€»æ˜¯è¿”å›ç‰¹å¾
        backbone=backbone,
        dinov2_model=getattr(opt, 'dinov2_model', 'vitb14'),
        freeze_dinov2=getattr(opt, 'freeze_dinov2', False),
        use_structure_aware=getattr(opt, 'use_structure_aware', False),
        use_hybrid=getattr(opt, 'use_hybrid', False),
        dropout=getattr(opt, 'dropout', 0.5),
        # é›¶åˆå§‹åŒ–å‚æ•°
        use_zero_init=getattr(opt, 'use_zero_init', False),
        use_zero_init_tri=getattr(opt, 'use_zero_init_tri', False),
        use_zero_init_detail=getattr(opt, 'use_zero_init_detail', False),
        use_zero_init_aff=getattr(opt, 'use_zero_init_aff', False),
    )


# ============================================================================
# â­ ä¿®æ”¹ make_convnext_model å‡½æ•°ï¼ˆå‘åå…¼å®¹ï¼‰
# ============================================================================

def make_convnext_model(
    num_class,
    block=4,
    return_f=False,
    resnet=False,
    # ========== â­ æ–°å¢å‚æ•°ï¼ˆé»˜è®¤ Falseï¼Œå®Œå…¨å‘åå…¼å®¹ï¼‰==========
    use_zero_init=False,
    use_zero_init_tri=False,
    use_zero_init_detail=False,
    use_zero_init_aff=False,
):
    """
    åˆ›å»º ConvNeXt/ResNet MCCG æ¨¡å‹ï¼ˆæ”¯æŒé›¶åˆå§‹åŒ–ï¼‰
    
    â­ å®Œå…¨å‘åå…¼å®¹ï¼š
    - ä¸ä¼ é›¶åˆå§‹åŒ–å‚æ•°æ—¶ï¼Œä½¿ç”¨æ ‡å‡† MCCG æ¨¡å‹
    - ä¼ å…¥é›¶åˆå§‹åŒ–å‚æ•°æ—¶ï¼Œä½¿ç”¨é›¶åˆå§‹åŒ–æ¨¡å‹
    
    Args:
        num_class: ç±»åˆ«æ•°
        block: MCCG åˆ†ç±»å™¨æ•°é‡
        return_f: æ˜¯å¦è¿”å›ç‰¹å¾
        resnet: æ˜¯å¦ä½¿ç”¨ ResNetï¼ˆFalse åˆ™ä½¿ç”¨ ConvNeXtï¼‰
        
        use_zero_init: â­ æ˜¯å¦å¯ç”¨é›¶åˆå§‹åŒ–
        use_zero_init_tri: â­ é›¶åˆå§‹åŒ– TripletAttention
        use_zero_init_detail: â­ é›¶åˆå§‹åŒ– DetailBranch
        use_zero_init_aff: â­ é›¶åˆå§‹åŒ– AFF
    
    Returns:
        model: æ¨¡å‹å®ä¾‹
    
    ä½¿ç”¨ç¤ºä¾‹:
        # æ ‡å‡†æ¨¡å‹ï¼ˆå‘åå…¼å®¹ï¼‰
        model = make_convnext_model(701, block=4)
        
        # é›¶åˆå§‹åŒ–æ¨¡å‹
        model = make_convnext_model(
            701, 
            block=4,
            use_zero_init=True,
            use_zero_init_tri=True
        )
    """
    
    # â­ å¦‚æœå¯ç”¨é›¶åˆå§‹åŒ–ï¼Œè°ƒç”¨é›¶åˆå§‹åŒ–åˆ›å»ºå‡½æ•°
    if use_zero_init:
        return make_model_with_zero_init(
            num_class=num_class,
            block=block,
            return_f=return_f,
            backbone='resnet' if resnet else 'convnext',
            use_zero_init=use_zero_init,
            use_zero_init_tri=use_zero_init_tri,
            use_zero_init_detail=use_zero_init_detail,
            use_zero_init_aff=use_zero_init_aff,
        )
    
    # â­ æ ‡å‡†æ¨¡å‹ï¼šè°ƒç”¨åŸæœ‰çš„ build_convnext
    print("="*70)
    print(f"Building MCCG with {'ResNet101' if resnet else 'ConvNeXt-Tiny'} backbone")
    print("="*70)
    print("===========building convnext===========")
    
    model = build_convnext(
        num_classes=num_class,  # â­ æ³¨æ„è¿™é‡Œæ˜¯ num_classesï¼ˆå¸¦ sï¼‰
        block=block,
        return_f=return_f,
        resnet=resnet
    )
    
    return model

# ============================================================================
# â­ å¦‚æœä½ è¿˜æœ‰ build_mccg_model å‡½æ•°ï¼Œä¹Ÿéœ€è¦ä¿®æ”¹
# ============================================================================

def build_mccg_model(
    num_classes,
    block=4,
    return_f=False,
    backbone='convnext',
    dinov2_model='vitb14',
    freeze_dinov2=False,
    use_structure_aware=False,
    use_hybrid=False,
    dropout=0.5,
    # ========== â­ æ–°å¢å‚æ•° ==========
    use_zero_init=False,
    use_zero_init_tri=False,
    use_zero_init_detail=False,
    use_zero_init_aff=False,
):
    """
    å·¥å‚å‡½æ•°ï¼šåˆ›å»º MCCG æ¨¡å‹ï¼ˆæ”¯æŒé›¶åˆå§‹åŒ–ï¼‰
    """
    return make_model_with_zero_init(
        num_class=num_classes,  # â­ æ³¨æ„å‚æ•°åè½¬æ¢
        block=block,
        return_f=return_f,
        backbone=backbone,
        dinov2_model=dinov2_model,
        freeze_dinov2=freeze_dinov2,
        use_structure_aware=use_structure_aware,
        use_hybrid=use_hybrid,
        dropout=dropout,
        use_zero_init=use_zero_init,
        use_zero_init_tri=use_zero_init_tri,
        use_zero_init_detail=use_zero_init_detail,
        use_zero_init_aff=use_zero_init_aff,
    )


# ============================================================================
# â­â­â­ æµ‹è¯•å’ŒéªŒè¯ä»£ç  â­â­â­
# ============================================================================

if __name__ == '__main__':
    """
    æµ‹è¯•ä»£ç ï¼šéªŒè¯æ‰€æœ‰å‡½æ•°æ­£å¸¸å·¥ä½œ
    """
    import torch
    from argparse import Namespace
    
    print("="*80)
    print("Testing make_model.py functions")
    print("="*80)
    
    # æµ‹è¯• 1: æ ‡å‡†æ¨¡å‹
    print("\n[Test 1] Standard ConvNeXt model:")
    model = make_convnext_model(num_class=701, block=2, return_f=True, resnet=False)
    x = torch.randn(2, 3, 256, 256)
    x2 = torch.randn(2, 3, 256, 256)
    model.train()
    out = model(x, x2)
    print(f"âœ… Output type: {type(out)}")
    
    # æµ‹è¯• 2: é›¶åˆå§‹åŒ–æ¨¡å‹
    print("\n[Test 2] Zero-Init model:")
    try:
        model = make_convnext_model(
            num_class=701, 
            block=2, 
            return_f=True,
            use_zero_init=True,
            use_zero_init_tri=True
        )
        out = model(x, x2)
        print(f"âœ… Zero-Init model works: {type(out)}")
    except Exception as e:
        print(f"âŒ Zero-Init failed: {e}")
    
    # æµ‹è¯• 3: make_model_from_opt
    print("\n[Test 3] make_model_from_opt:")
    opt = Namespace(
        nclasses=701,
        block=2,
        triplet_loss=0.3,
        dinov2=False,
        resnet=False,
        use_zero_init=False
    )
    model = make_model_from_opt(opt)
    print(f"âœ… Model created from opt")
    
    print("\n" + "="*80)
    print("All tests passed! âœ…")
    print("="*80)
